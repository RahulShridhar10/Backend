{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_image(input_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Preprocess a single image with skew correction, line removal, \n",
    "    noise reduction, and adaptive thresholding.\n",
    "    \n",
    "    :param input_path: Path to the input image\n",
    "    :param output_path: Path to save the preprocessed image (optional)\n",
    "    :return: Preprocessed image\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    image = cv2.imread(input_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Error reading image from {input_path}\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Remove horizontal & vertical lines\n",
    "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (50, 1))\n",
    "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 50))\n",
    "    remove_horizontal = cv2.morphologyEx(gray, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\n",
    "    remove_vertical = cv2.morphologyEx(gray, cv2.MORPH_OPEN, vertical_kernel, iterations=2)\n",
    "\n",
    "    # Find contours of horizontal and vertical lines\n",
    "    cnts_h = cv2.findContours(remove_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "    cnts_v = cv2.findContours(remove_vertical, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "    # Draw white over the line contours to remove them\n",
    "    for c in cnts_h + cnts_v:\n",
    "        cv2.drawContours(image, [c], -1, (255, 255, 255), 5)\n",
    "\n",
    "    # Apply noise reduction and adaptive thresholding\n",
    "    blurred = cv2.medianBlur(gray, 3)  # Noise reduction\n",
    "    filtered = cv2.fastNlMeansDenoising(blurred, None, 30, 7, 21)\n",
    "    final_img = cv2.adaptiveThreshold(filtered, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 15, 5)\n",
    "\n",
    "    # Save the preprocessed image if output path is provided\n",
    "    if output_path:\n",
    "        cv2.imwrite(output_path, final_img)\n",
    "        print(f\"Preprocessed image saved to: {output_path}\")\n",
    "\n",
    "    return final_img\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_image = r\"C:\\Users\\91948\\Desktop\\final\\053.png\"  # Replace with your image path\n",
    "    output_image = r\"C:\\Users\\91948\\Desktop\\final\\53_pre.png\"  # Replace with desired output path\n",
    "    \n",
    "    preprocessed = preprocess_image(input_image, output_image)\n",
    "    \n",
    "    # Optionally display the preprocessed image\n",
    "    cv2.imshow(\"Preprocessed Image\", preprocessed)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "\n",
    "image = cv2.imread(r\"C:\\Users\\91948\\Desktop\\final\\53_pre.png\")\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "ret,thresh1 = cv2.threshold(gray ,127,255,cv2.THRESH_BINARY_INV)\n",
    "\n",
    "dilate = cv2.dilate(thresh1, None, iterations=2)\n",
    "\n",
    "cnts = cv2.findContours(dilate.copy(), cv2.RETR_EXTERNAL,\n",
    "    cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] #if imutils.is_cv2() else cnts[1]\n",
    "\n",
    "for i, cnt in enumerate(cnts):\n",
    "    print(f\"Contour {i}: type={type(cnt)}, shape={cnt.shape if hasattr(cnt, 'shape') else 'No shape'}, dtype={cnt.dtype if hasattr(cnt, 'dtype') else 'No dtype'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Output path for the processed image\n",
    "output_path = r\"C:\\Users\\91948\\Desktop\\final\\53_cleaned.png\"\n",
    "\n",
    "# Assuming 'image' and 'cnts' are already defined\n",
    "# 'image' is your original image\n",
    "# 'cnts' is your list of contours\n",
    "\n",
    "# Make a copy of the original image for processing\n",
    "processed_image = image.copy()\n",
    "\n",
    "# Fix the contours format if needed\n",
    "fixed_cnts = []\n",
    "for cnt in cnts:\n",
    "    if cnt.shape[1] == 4:  # If shape is (n, 4) instead of (n, 1, 2)\n",
    "        # Take only the first two columns (x,y coordinates)\n",
    "        fixed_cnt = cnt[:, :2].reshape(-1, 1, 2)\n",
    "        fixed_cnts.append(fixed_cnt)\n",
    "    else:\n",
    "        fixed_cnts.append(cnt)\n",
    "\n",
    "# Sort the contours \n",
    "sorted_ctrs = sorted(fixed_cnts, key=lambda ctr: cv2.boundingRect(ctr)[0] + cv2.boundingRect(ctr)[1] * image.shape[1])\n",
    "\n",
    "# Process each contour\n",
    "i = 0\n",
    "for cnt in sorted_ctrs:\n",
    "    # Check the area of contour, if it is very small ignore it\n",
    "    if cv2.contourArea(cnt) < 0:\n",
    "        continue\n",
    "\n",
    "    # Get bounding rectangle for the contour\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    \n",
    "    # Filter out contours that are not within our desired dimensions\n",
    "    if not (w > 0 and h > 0 and w < 20 and h < 20):\n",
    "        continue\n",
    "    \n",
    "    # Create a mask for the current contour\n",
    "    mask = np.zeros(processed_image.shape[:2], dtype=np.uint8)\n",
    "    cv2.drawContours(mask, [cnt], 0, 255, -1)\n",
    "    \n",
    "    # Replace the contour area with white pixels\n",
    "    processed_image[mask == 255] = [255, 255, 255]\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# Save the processed image directly to the specified path\n",
    "cv2.imwrite(output_path, processed_image)\n",
    "\n",
    "print(f\"Processed {i} contours and saved processed image to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read image and convert to grayscale\n",
    "img = cv2.imread(r\"C:\\Users\\91948\\Desktop\\final\\53_cleaned.png\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply adaptive thresholding\n",
    "thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                              cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "# Functions for projection profiles\n",
    "def getHorizontalProjectionProfile(image):\n",
    "    binary = np.where(image > 0, 1, 0)\n",
    "    horizontal_projection = np.sum(binary, axis=1)\n",
    "    return horizontal_projection\n",
    "\n",
    "def getVerticalProjectionProfile(image):\n",
    "    binary = np.where(image > 0, 1, 0)\n",
    "    vertical_projection = np.sum(binary, axis=0)\n",
    "    return vertical_projection\n",
    "\n",
    "# Get horizontal projection profile\n",
    "h_projection = getHorizontalProjectionProfile(thresh)\n",
    "\n",
    "# Parameters for horizontal segmentation\n",
    "h_min_line_height = 10\n",
    "h_min_line_gap = 3\n",
    "h_threshold = np.mean(h_projection) * 0.3\n",
    "\n",
    "# Detect horizontal line boundaries\n",
    "h_line_boundaries = []\n",
    "in_line = False\n",
    "start_line = 0\n",
    "\n",
    "for i in range(len(h_projection)):\n",
    "    if not in_line and h_projection[i] > h_threshold:\n",
    "        in_line = True\n",
    "        start_line = i\n",
    "    elif in_line and (h_projection[i] <= h_threshold or i == len(h_projection) - 1):\n",
    "        end_line = i\n",
    "        if end_line - start_line >= h_min_line_height:\n",
    "            h_line_boundaries.append((start_line, end_line))\n",
    "        in_line = False\n",
    "\n",
    "# Merge horizontal lines that are very close together\n",
    "merged_h_lines = []\n",
    "i = 0\n",
    "while i < len(h_line_boundaries):\n",
    "    current_start, current_end = h_line_boundaries[i]\n",
    "\n",
    "    while i + 1 < len(h_line_boundaries) and h_line_boundaries[i+1][0] - current_end < h_min_line_gap:\n",
    "        i += 1\n",
    "        _, current_end = h_line_boundaries[i]\n",
    "\n",
    "    merged_h_lines.append((current_start, current_end))\n",
    "    i += 1\n",
    "\n",
    "# Find the largest horizontal segment based on height\n",
    "largest_h_segment = None\n",
    "largest_h_height = 0\n",
    "\n",
    "for start, end in merged_h_lines:\n",
    "    height = end - start\n",
    "    if height > largest_h_height:\n",
    "        largest_h_height = height\n",
    "        largest_h_segment = (start, end)\n",
    "\n",
    "# If no horizontal segments found, use the entire image\n",
    "if largest_h_segment is None:\n",
    "    largest_h_segment = (0, img.shape[0])\n",
    "\n",
    "# Extract the largest horizontal segment with padding\n",
    "padding_h = 5\n",
    "y_start = max(0, largest_h_segment[0] - padding_h)\n",
    "y_end = min(img.shape[0], largest_h_segment[1] + padding_h)\n",
    "\n",
    "h_segment_img = img[y_start:y_end, :]\n",
    "h_segment_thresh = thresh[y_start:y_end, :]\n",
    "\n",
    "# Apply vertical projection to the largest horizontal segment\n",
    "v_projection = getVerticalProjectionProfile(h_segment_thresh)\n",
    "\n",
    "# Parameters for vertical segmentation\n",
    "v_min_column_width = 10\n",
    "v_min_column_gap = 3\n",
    "v_threshold = np.mean(v_projection) * 0.3\n",
    "\n",
    "# Detect vertical column boundaries\n",
    "v_column_boundaries = []\n",
    "in_column = False\n",
    "start_column = 0\n",
    "\n",
    "for i in range(len(v_projection)):\n",
    "    if not in_column and v_projection[i] > v_threshold:\n",
    "        in_column = True\n",
    "        start_column = i\n",
    "    elif in_column and (v_projection[i] <= v_threshold or i == len(v_projection) - 1):\n",
    "        end_column = i\n",
    "        if end_column - start_column >= v_min_column_width:\n",
    "            v_column_boundaries.append((start_column, end_column))\n",
    "        in_column = False\n",
    "\n",
    "# Merge vertical columns that are very close together\n",
    "merged_v_columns = []\n",
    "i = 0\n",
    "while i < len(v_column_boundaries):\n",
    "    current_start, current_end = v_column_boundaries[i]\n",
    "\n",
    "    while i + 1 < len(v_column_boundaries) and v_column_boundaries[i+1][0] - current_end < v_min_column_gap:\n",
    "        i += 1\n",
    "        _, current_end = v_column_boundaries[i]\n",
    "\n",
    "    merged_v_columns.append((current_start, current_end))\n",
    "    i += 1\n",
    "\n",
    "# Find the largest vertical segment based on width\n",
    "largest_v_segment = None\n",
    "largest_v_width = 0\n",
    "\n",
    "for start, end in merged_v_columns:\n",
    "    width = end - start\n",
    "    if width > largest_v_width:\n",
    "        largest_v_width = width\n",
    "        largest_v_segment = (start, end)\n",
    "\n",
    "# If no vertical segments found, use the entire width\n",
    "if largest_v_segment is None:\n",
    "    largest_v_segment = (0, img.shape[1])\n",
    "\n",
    "# Extract the largest vertical segment with padding\n",
    "padding_v = 5\n",
    "x_start = max(0, largest_v_segment[0] - padding_v)\n",
    "x_end = min(img.shape[1], largest_v_segment[1] + padding_v)\n",
    "\n",
    "# Extract the final text region (intersection of largest horizontal and vertical segments)\n",
    "final_text_img = img[y_start:y_end, x_start:x_end]\n",
    "\n",
    "# Output path for the extracted image\n",
    "output_path = r\"C:\\Users\\91948\\Desktop\\final\\53_extracted.jpg\"\n",
    "\n",
    "# Save the final text region with the specified filename\n",
    "cv2.imwrite(output_path, final_text_img)\n",
    "\n",
    "print(f\"Extracted text region saved to {output_path}\")\n",
    "print(f\"Final text region dimensions: {final_text_img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def super_resolve_image(input_image_path, output_folder):\n",
    "    \"\"\"\n",
    "    Perform super-resolution on a single image using the specified model.\n",
    "    \n",
    "    Args:\n",
    "    input_image_path (str): Full path to the input image\n",
    "    output_folder (str): Folder where the super-resolved image will be saved\n",
    "    \"\"\"\n",
    "    command = [\n",
    "        \"python\", \n",
    "        r\"C:\\Users\\91948\\Desktop\\final\\super_res_mod.py\",\n",
    "        \"-m\", r\"C:\\Users\\91948\\Desktop\\final\\FSRCNN_x4.pb\",\n",
    "        \"-i\", input_image_path,\n",
    "        \"-o\", output_folder\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Run the super-resolution command\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "        \n",
    "        # Check if the command was successful\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Successfully processed image: {input_image_path}\")\n",
    "        else:\n",
    "            print(f\"Error processing image: {input_image_path}\")\n",
    "            print(f\"Error output: {result.stderr}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the path to your single input image\n",
    "    input_image = r\"C:\\Users\\91948\\Desktop\\final\\53_extracted.jpg\"\n",
    "    \n",
    "    # Specify the output folder\n",
    "    output_folder = r\"C:\\Users\\91948\\Desktop\\final\"\n",
    "    \n",
    "    # Call the function to super-resolve the image\n",
    "    super_resolve_image(input_image, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "class PalmLeafCharacterProcessor:\n",
    "    def __init__(self, model_path, class_indices_path, ground_truth_dir):\n",
    "        \"\"\"\n",
    "        Initialize the processor with model, class indices, and ground truth directory.\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to the trained Keras model\n",
    "            class_indices_path (str): Path to the class indices JSON file\n",
    "            ground_truth_dir (str): Directory containing ground truth character images\n",
    "        \"\"\"\n",
    "        # Load the model\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Load class indices\n",
    "        with open(class_indices_path, 'r') as f:\n",
    "            class_indices = json.load(f)\n",
    "        \n",
    "        # Reverse the class indices dictionary\n",
    "        self.class_names = {v: k for k, v in class_indices.items()}\n",
    "        \n",
    "        # Set image parameters\n",
    "        self.IMG_SIZE = (64, 64)\n",
    "        \n",
    "        # Set ground truth directory\n",
    "        self.ground_truth_dir = ground_truth_dir\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Preprocess image for model prediction.\n",
    "        \n",
    "        Args:\n",
    "            image_path (str): Path to the image\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Preprocessed image array\n",
    "        \"\"\"\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.resize(img, self.IMG_SIZE)\n",
    "        img_array = img.astype('float32') / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        return img_array\n",
    "\n",
    "    def predict_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Predict class for a single image.\n",
    "        \n",
    "        Args:\n",
    "            image_path (str): Path to the image\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Predicted class name and confidence\n",
    "        \"\"\"\n",
    "        preprocessed_img = self.preprocess_image(image_path)\n",
    "        prediction = self.model.predict(preprocessed_img)\n",
    "        predicted_class_index = np.argmax(prediction)\n",
    "        predicted_class_name = self.class_names[predicted_class_index]\n",
    "        confidence = prediction[0][predicted_class_index]\n",
    "        return predicted_class_name, confidence\n",
    "\n",
    "    def preprocess_input_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Preprocess the input palm leaf image for character extraction.\n",
    "        \n",
    "        Args:\n",
    "            image_path (str): Path to the input image\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Original image and binary image\n",
    "        \"\"\"\n",
    "        # Read the input image\n",
    "        input_image = cv2.imread(image_path)\n",
    "        if input_image is None:\n",
    "            raise ValueError(f\"Error: Could not read the image at {image_path}\")\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        grayscale_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply Gaussian blur to reduce noise\n",
    "        blurred_image = cv2.GaussianBlur(grayscale_image, (5, 5), 0)\n",
    "        \n",
    "        # Apply adaptive thresholding to create binary image\n",
    "        binary_image = cv2.adaptiveThreshold(\n",
    "            blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            cv2.THRESH_BINARY_INV, 31, 15\n",
    "        )\n",
    "        \n",
    "        return input_image, binary_image\n",
    "\n",
    "    def refine_segmentation(self, binary_image):\n",
    "        \"\"\"\n",
    "        Refine the binary image using morphological operations.\n",
    "        \n",
    "        Args:\n",
    "            binary_image (numpy.ndarray): Input binary image\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Refined binary image\n",
    "        \"\"\"\n",
    "        # Create a small kernel for morphological operations\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        \n",
    "        # Dilate the image to connect nearby character components\n",
    "        morphed_image = cv2.dilate(binary_image, kernel, iterations=1)\n",
    "        \n",
    "        return morphed_image\n",
    "\n",
    "    def filter_components(self, binary_image):\n",
    "        \"\"\"\n",
    "        Filter out small noise components based on area.\n",
    "        \n",
    "        Args:\n",
    "            binary_image (numpy.ndarray): Input binary image\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Filtered binary image\n",
    "        \"\"\"\n",
    "        # Find connected components\n",
    "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary_image, connectivity=8)\n",
    "        \n",
    "        # Create an empty image for filtered components\n",
    "        filtered_binary = np.zeros_like(binary_image)\n",
    "        \n",
    "        # Calculate average component area (excluding background)\n",
    "        avg_area = np.mean(stats[1:, cv2.CC_STAT_AREA])\n",
    "        \n",
    "        # Filter components based on area\n",
    "        for i in range(1, num_labels):\n",
    "            if stats[i, cv2.CC_STAT_AREA] > avg_area * 0.5:  # Remove small noise\n",
    "                filtered_binary[labels == i] = 255\n",
    "        \n",
    "        return filtered_binary\n",
    "\n",
    "    def extract_and_replace_characters(self, input_image, binary_image, \n",
    "                                       min_height=12, max_height=40, \n",
    "                                       min_width=12, max_width=55):\n",
    "        \"\"\"\n",
    "        Extract character components, predict their class, and replace with ground truth.\n",
    "        \n",
    "        Args:\n",
    "            input_image (numpy.ndarray): Original input image\n",
    "            binary_image (numpy.ndarray): Binary image\n",
    "            min_height (int): Minimum character height\n",
    "            max_height (int): Maximum character height\n",
    "            min_width (int): Minimum character width\n",
    "            max_width (int): Maximum character width\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Image with replaced characters, number of replaced characters, and total contours meeting criteria\n",
    "        \"\"\"\n",
    "        # Create a copy of the input image to modify\n",
    "        output_image = input_image.copy()\n",
    "        \n",
    "        # Counters for replaced and total characters\n",
    "        replaced_characters_count = 0\n",
    "        total_contours_meeting_criteria = 0\n",
    "        \n",
    "        # Find contours in the binary image\n",
    "        contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Iterate through contours\n",
    "        for idx, contour in enumerate(contours):\n",
    "            # Get bounding rectangle\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            \n",
    "            # Check if the component meets size criteria\n",
    "            if min_height <= h <= max_height and min_width <= w <= max_width:\n",
    "                # Increment total contours meeting criteria\n",
    "                total_contours_meeting_criteria += 1\n",
    "                \n",
    "                # Extract the character component\n",
    "                character = input_image[y:y+h, x:x+w]\n",
    "                \n",
    "                # Use a context manager to create and remove a temporary file\n",
    "                with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:\n",
    "                    temp_path = temp_file.name\n",
    "                    cv2.imwrite(temp_path, character)\n",
    "                \n",
    "                try:\n",
    "                    # Predict the character\n",
    "                    predicted_class, confidence = self.predict_image(temp_path)\n",
    "                    \n",
    "                    # Find ground truth image\n",
    "                    ground_truth_path = os.path.join(self.ground_truth_dir, f\"{predicted_class}.png\")\n",
    "                    \n",
    "                    if os.path.exists(ground_truth_path):\n",
    "                        # Read ground truth image\n",
    "                        ground_truth_img = cv2.imread(ground_truth_path)\n",
    "                        \n",
    "                        # Resize ground truth to match original contour size\n",
    "                        ground_truth_resized = cv2.resize(ground_truth_img, (w, h))\n",
    "                        \n",
    "                        # Replace the original contour with ground truth\n",
    "                        output_image[y:y+h, x:x+w] = ground_truth_resized\n",
    "                        \n",
    "                        # Increment replaced characters count\n",
    "                        replaced_characters_count += 1\n",
    "                    \n",
    "                    # Print prediction details\n",
    "                    print(f\"Contour {idx} - Predicted: {predicted_class} (Confidence: {confidence:.2f})\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing character: {e}\")\n",
    "                \n",
    "                # Remove temporary file\n",
    "                finally:\n",
    "                    os.unlink(temp_path)\n",
    "        \n",
    "        # Print total contours meeting criteria\n",
    "        print(f\"Total contours meeting size criteria: {total_contours_meeting_criteria}\")\n",
    "        \n",
    "        return output_image, replaced_characters_count, total_contours_meeting_criteria\n",
    "\n",
    "    def process_palm_leaf(self, input_image_path, visualize=False):\n",
    "        \"\"\"\n",
    "        Main method to process palm leaf image.\n",
    "        \n",
    "        Args:\n",
    "            input_image_path (str): Path to the input palm leaf image\n",
    "            visualize (bool): Whether to display visualization\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Processed image with replaced characters, \n",
    "                   number of replaced characters, \n",
    "                   and total contours meeting criteria\n",
    "        \"\"\"\n",
    "        # Preprocess the image\n",
    "        input_image, binary_image = self.preprocess_input_image(input_image_path)\n",
    "        \n",
    "        # Refine segmentation\n",
    "        refined_image = self.refine_segmentation(binary_image)\n",
    "        \n",
    "        # Filter out noise\n",
    "        filtered_image = self.filter_components(refined_image)\n",
    "        \n",
    "        # Extract and replace characters\n",
    "        processed_image, num_replaced, total_contours = self.extract_and_replace_characters(\n",
    "            input_image, filtered_image,\n",
    "            min_height=30, max_height=170,\n",
    "            min_width=30, max_width=170\n",
    "        )\n",
    "        \n",
    "        # Visualize if requested\n",
    "        if visualize:\n",
    "            plt.figure(figsize=(15, 10))\n",
    "            plt.imshow(cv2.cvtColor(processed_image, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(\"Processed Palm Leaf Characters\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        \n",
    "        return processed_image, num_replaced, total_contours\n",
    "\n",
    "def main():\n",
    "    # Configuration paths - replace these with your actual paths\n",
    "    MODEL_PATH = r\"C:\\Users\\91948\\Desktop\\final\\palm_model.keras\"\n",
    "    CLASS_INDICES_PATH = r\"C:\\Users\\91948\\Desktop\\final\\class_indices.json\"\n",
    "    GROUND_TRUTH_DIR = r\"C:\\Users\\91948\\Desktop\\final\\ground_truth\"\n",
    "    INPUT_IMAGE_PATH = r\"C:\\Users\\91948\\Desktop\\final\\53_extracted_fsrcnn_x4.jpg\"\n",
    "    OUTPUT_IMAGE_PATH = r\"C:\\Users\\91948\\Desktop\\final\\53_processed.png\"\n",
    "\n",
    "    # Create processor\n",
    "    processor = PalmLeafCharacterProcessor(\n",
    "        MODEL_PATH, \n",
    "        CLASS_INDICES_PATH, \n",
    "        GROUND_TRUTH_DIR\n",
    "    )\n",
    "\n",
    "    # Process the palm leaf image\n",
    "    processed_image, num_replaced, total_contours = processor.process_palm_leaf(\n",
    "        INPUT_IMAGE_PATH, \n",
    "        visualize=True\n",
    "    )\n",
    "\n",
    "    # Save the processed image\n",
    "    cv2.imwrite(OUTPUT_IMAGE_PATH, processed_image)\n",
    "    print(f\"Number of characters replaced: {num_replaced}\")\n",
    "    print(f\"Total contours meeting criteria: {total_contours}\")\n",
    "    print(f\"Processed image saved to {OUTPUT_IMAGE_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
